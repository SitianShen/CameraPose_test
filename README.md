# CameraPose_test
Test the multi-view camera pose generation.

The camera pose of the input image of Zero123++ (`data/edited_chair.png`) is:
``"rotation": 0.012566370614359171,
            "transform_matrix": [
                [
                    0.3260957896709442,
                    0.14048941433429718,
                    -0.934839129447937,
                    -3.7684571743011475
                ],
                [
                    -0.9453368186950684,
                    0.04846210405230522,
                    -0.3224746286869049,
                    -1.2999367713928223
                ],
                [
                    0.0,
                    0.9888953566551208,
                    0.1486130952835083,
                    0.5990785360336304
                ],
                [
                    0.0,
                    0.0,
                    0.0,
                    1.0
                ]
            ]``

Six-view images generated by Zero123++:
`data/chair_6views/train`
Their elevation and azimuth are shown in Zero123++ paper.

Conda env: use the same packages as gaussian-splatting.

Put the pretrained chair ply file in this folder:
`output/chair_initial/point_cloud/iteration_30000`

Camera pose file:
`data/chair_6views/transforms_train.json`

To validate the correction of camera pose, finetune pre-trained gaussian:

``python train.py --iterations 5000 -s data/chair_6views -m output/chair_initial --finetuning True``

The output will be in this folder: `output/chair_initial/finetuned`

To render the finetuned gaussian, first move the finetuned model folder (`output/chair_initial/finetuned/iteration_5000`) from `output/chair_initial/finetuned` to `output/chair_initial/output`, then run:

``python render.py -m output/lego_initial --iteration 5000``

To render the initial gaussian, run:

``python render.py -m output/lego_initial --iteration 30000``

Then the render images will be in:
`output/chair_initial/train/ours_5000/renders`
